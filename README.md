En jobs dejo mi job de hadoop y de hive.

 - El job de hadoop es un simple Contar cuÃ¡ntas reseÃ±as fueron positivas y cuÃ¡ntas negativas segÃºn el campo recommended.
AsÃ­ que el job responde a la pregunta:

â€œÂ¿CuÃ¡ntas reseÃ±as totales recomiendan el juego y cuÃ¡ntas no lo recomiendan?â€

âš™ï¸ 2ï¸âƒ£ El Mapper (mapper.py)

Cada lÃ­nea del CSV (una reseÃ±a) pasa por el mapper.

Se separan las columnas con el mÃ³dulo csv de Python.

Se toma la columna nÃºmero 7 (fields[7]), que corresponde a recommended.

Si ese valor es "true" o "false", el mapper emite una claveâ€“valor asÃ­:

true    1
false   1


ðŸ‘‰ En otras palabras, cada reseÃ±a genera una lÃ­nea con su valor de â€œrecomendadoâ€ y el nÃºmero 1.

âš™ï¸ 3ï¸âƒ£ El Reducer (reducer.py)

El reducer recibe las salidas del mapper agrupadas por clave (true o false) y suma todos los valores.

AsÃ­ calcula cuÃ¡ntas veces apareciÃ³ cada clave.

Por ejemplo, si el mapper produjo:

true    1
true    1
false   1
true    1
false   1


El reducer sumarÃ¡ y emitirÃ¡:

false   2
true    3

ðŸ“Š 4ï¸âƒ£ Tu salida final
false   187
true    1357


ðŸ’¡ Esto significa:

187 reseÃ±as negativas (no recomendaron el juego)

1357 reseÃ±as positivas (sÃ­ recomendaron el juego)

Total = 1544 reseÃ±as procesadas.



 - El job de hive crea una tabla games con los juegos Ãºnicos (con las columnas app_id, y app_name), crea una tabla game_recommendation que resume para cada juego sus reseÃ±as, y sus respectivas recommended, voted_helpful y voted_funny

'''
jobs
-> hadoop
  -> mapper.py
  -> reducer.py
  -> output
    -> success
    -> parte-00000
    -> parte-00001
    -> parte-00002

->hive
  -> success
  -> dsfnjdsfafsda // defrente los output


 '''
